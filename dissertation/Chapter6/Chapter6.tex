% !TEX root =  ../report.tex

\section{Evaluation}
\label{s:eval}

This project was intended as an investigation, and therefore it can certainly be considered successful, despite not achieving the speed-up that was hoped for at the outset. Through the contributions made to the OP2 project while completing this project, important groundwork has been layed for future contributors to build on top, and implement further optimisations and run-time assertions which might achieve some speedup at run-time. \par Furthermore, it is useful to discover that the technique of defining constants for the preprocessor does not sufficiently reduce the run-time duration to justify the run-time re-compilation. This will inform future investiagations into what techniques should implemented.

\subsection{Future Work}
\label{ss:fw}

\subsubsection{Run-Time Assertions}
As previously mentioned, it seems necessary for more to be assertions to be made at run-time in order to produce actual speed-up after JIT compilation. There are a number of possible loop optimisations which could be made, including identifying a loop inside a kernel, and at run-time having the loop bound be hard-coded to remove the need to evaluate the expression of every iteration; or more complex optimisation where two separate parallel loops might be provably able to be fused into a single loop, but only if the inputs allow for it - meaning this could only be done at run-time.

\begin{wrapfigure}[8]{l}{.4\textwidth}
  \vspace{-2em}
  \centering
  \caption{2D Loop Tiling}
  \label{fig:tile2D}
  \begin{tikzpicture}
      [%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          box/.style={rectangle,draw=black,thick, minimum size=.4cm},
          scale=0.4
      ]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \foreach \x in {0,...,9}{
      \foreach \y in {0,...,9}
          \node[box] at (\x,\y){};
  }

  \foreach \x in {0,...,3}{
      \foreach \y in {6,...,9}
        \node[box,fill=red] at (\x,\y){};
  }

  \foreach \x in {4,...,7}{
      \foreach \y in {6,...,9}
        \node[box,fill=blue] at (\x,\y){};
  }

  \foreach \x in {8,...,9}{
      \foreach \y in {6,...,9}
        \node[box,fill=green] at (\x,\y){};
  }

  \foreach \x in {0,...,3}{
      \foreach \y in {2,...,5}
        \node[box,fill=green] at (\x,\y){};
  }

  \foreach \x in {4,...,7}{
      \foreach \y in {2,...,5}
        \node[box,fill=red] at (\x,\y){};
  }

  \foreach \x in {8,...,9}{
      \foreach \y in {2,...,5}
        \node[box,fill=blue] at (\x,\y){};
  }

  \foreach \x in {0,...,3}{
      \foreach \y in {0,...,1}
        \node[box,fill=blue] at (\x,\y){};
  }

  \foreach \x in {4,...,7}{
      \foreach \y in {0,...,1}
        \node[box,fill=green] at (\x,\y){};
  }

  \foreach \x in {8,...,9}{
      \foreach \y in {0,...,1}
        \node[box,fill=red] at (\x,\y){};
  }

  \draw[<->] (10,6) -- (10,9);
  \node[anchor=south west] at (10,7) {$n$};
  \draw[<->] (11,0) -- (11,9);
  \node[anchor=south west] at (11,4) {$N$};

  \end{tikzpicture}
\end{wrapfigure}

\noindent There is also research into applying loop tiling to the generated code, which is dividing the iterations of a loop into sub-regions where both temporal and spatial locality in memory can be exploited.\\
For example, a loop iterating over a 2D array of size $N \times N$, with Level 1 (L1) cache size of $n$ such that $n < N$ would benefit from dividing the array into squares of size at most $n \times n$, as long as this does not violate any data dependencies in the order of operations (see Figure \ref{fig:tile2D}). Doing so prevents values from being evicted from L1 cache prior to being needed again.
\par
Currently this has only been applied to OPS \cite{opstiling}, the precursor to OP2 \cite{opsmain}, which supports structured mesh solvers only. There does exist a 2019 paper \cite{slope} on automated loop tiling for unstructed meshes, and the issues posed by the need for indirect array accesses. A library provided which demonstrates the technique \cite{SLOPErep}, including a demo using the same \textit{airfoil} application used for this report.
\par
During this project it was suggested that applying loop tiling inside the JIT compilation stage could be a good extension, and would likely provide speedup, however unfortunately there was not sufficient time to reasonably expect the functionality to be finished.

\subsubsection{CUDA JIT Compilation}
Going in a different direction, the CUDA library does provide an interface for JIT compilation natively, which would allow for re-compilation without requiring a system call to \verb|make| for every loop kernel. System calls can be a significant bottleneck in some cases, and this problem would only compound for applications with a large number of parallel loops. Therefore, using the CUDA JIT compilation system would likely bring down the upfront cost of recompilation. For \textit{airfoil} this re-compile time is very low, it would not have much impact on the results gathered.
\par
Using CUDA's native JIT compilation pipeline would provide the added benefit that a application developer using OP2 would not have to write the Makefile themselves, as currently its contents are not generated by the OP2 code generator, but simply relies on the executable producing an error if a Makefile with the correct target does not exist.

\subsubsection{Alternative Hardware Targets}
Finally, there are other hardware targets supported by OP2 which may be able to benefit from Just-In-Time compilation, and since the purpose of OP2 is to provide performance on multiple hardware platforms from a single application code any new optimisation which is found to improve performance, should also be ported to other platforms where it might be able to provide benefit. Any users who do not  primarly utilise Nvidia GPU hardware should benefit from the JIT compilation optimisation.

\clearpage
\subsection{Project Management}
\label{ss:pm}
This Section serves as a reflection on the project as a whole, and how I believe it went. If this is not of interest the report Conclusion on page \pageref{s:conc}.
\par
The Gantt chart in figure \ref{progGantt} was produced for the progress report submitted in November, 6 weeks into the project. Having now completed the project I can reflect on how well the timeline was followed, and the successes and challenges of each of the four periods.
\newcommand\w{25}
\begin{figure}[h]
\centering
\makebox[0pt]{
\begin{ganttchart}[
expand chart=1.22\textwidth,
vgrid={*{1}dotted, *4{white},*1{dotted}, *{12}{white}, *1{dotted}, *1{white}, *1{dotted}, *5{white}},
hgrid=true,
y unit chart=0.8cm,
inline,
today=6,
today label=Progress Report,
today label font=\itshape,
]{0}{\w}

 \gantttitle{Timeline}{26} \\
 \gantttitlelist{0,...,\w}{1} \\

 \ganttgroup{Research}{1}{5} \\ %0%

 \ganttset{inline=false}
 \ganttbar{Read Papers}{1}{2}          %1%
 \ganttset{inline=true}

 \ganttgroup{Implementation}{6}{18} \\ %2%

 \ganttset{inline=false}
 \ganttbar{Familiarity Work}{2}{5}     %3%
 \ganttset{inline=true}

 \ganttgroup{Benchmarking}{19}{20} \\  %4%

 \ganttset{inline=false}
 \ganttbar{Feature Development}{6}{8}  %5%
 \ganttbar{} {10}{12}                   %6%
 \ganttbar{} {14}{17}                  %7%
 \ganttset{inline=true}
 \ganttgroup{Documentation}{21}{25} \\ %8%

 \ganttset{inline=false}

 \ganttbar{Feature Testing}{9}{9}      %9%
 \ganttbar{}{13}{13}                   %10%
 \ganttbar{}{18}{18}                   %11%

\\

 \ganttbar{Results Gathering}{19}{19}  %12%
 \\
 \ganttbar{Results Processing}{19}{20} %13%
 \\

 \ganttbar{Final Report}{21}{25} \\    %14
 \ganttbar{Presentation}{21}{22}       %15

 \ganttlink{elem1}{elem3}
 \ganttlink{elem3}{elem5}
 \ganttlink{elem5}{elem9}
 \ganttlink{elem9}{elem6}
 \ganttlink{elem6}{elem10}
 \ganttlink{elem10}{elem7}
 \ganttlink{elem7}{elem11}
 \ganttlink{elem11}{elem12}
 \ganttset{link mid=0.25}
 \ganttlink{elem11}{elem13}
 \ganttset{link mid=0.5}
 \ganttlink{elem13}{elem14}
 \ganttset{link mid=0.25}
 \ganttlink{elem13}{elem15}
 \ganttset{link mid=0.5}
\end{ganttchart}
}
\caption{\label{progGantt} Gantt Diagram produced with Progress Report}
\end{figure}

\subsubsection{Challenges and Reflection}
\noindent \minititle{Research}
The research section of my project involved reading scientific papers, many produced by contributors to the OP2 framework; and hands-on with CUDA and OP2 trying to build familiarity before the actual implementation began. On reflection, My research was mostly focussed on the existing OP2 work, and many of my sources were from the same authors. Once I had already decided on my approach and begin implementation I discovered some similar work which might have influenced the direction of the project if I had been aware of them earlier on.

\minititle{Implementation}
The Implementation progressed largely as expected, with progress made at a good pace for the time allocated. I did find that while the timetable above listed a total of 3 full weeks for testing, partial solutions were difficult to test fully by as there was no executable generated to ensure the result was correct or perform much benchmarking until towards the end of the implementation.
\par
Instead testing during implementation relied more on comparing expected results from the code generation of airfoil, and modified versions of airfoil, with the actual outputs of the code generation scripts. For some areas of development, I foud it useful to write the code manually into the airfoil files, and figure out how it could work compiling the manually written code, and once it compiled successfully working backwards and modifying the code generator to produce equivalent code that would work for any application.
\par
I had discussed with my supervisor some extension work which could have been a part of the implementation if there was time, such as the Loop Tiling feature mentioned in previous Section \ref{ss:fw}. Since the core functionality of the project was only completed with 2 weeks of planned implementation time left, it was decided that this was an unreasonable goal, and it would be better to thoroughly benchmark the completed functionality than to attempt a complex extension feature and potentially leave it incomplete.

\minititle{Benchmarking}
It was fortunate that the decision to move on to benchmarking instead of pursuing further functionality was taken, as the original plan alloted only a week for gathering results and a second for analysing the results. In reality, the extra 2 weeks that had been allocated Implementation to were also required, as well as part of the time intended for making the project presentation (Week 21. Figure \ref{progGantt}).
\par
The delay was mostly due to the desire to use an HPC cluster to gather proper benchmarking data. While the graphics card in my personal laptop was sufficient for validating the code executed correctly, the results would likely have been noisy and inconsistent if not gathered on a dedicated system. Finding a cluster that would allow access to Kepler generation GPUs, and getting approved for access took longer than expected. With the benefit of hindsight it is clear that this process should have begun much earlier in the project, as it was always going to be necessary to have access to an HPC cluster.
\par
Eventually the Cambridge Service for Data-Driven Discovery (CSD3) kindly approved me to use their \textit{Wilkes2} GPU cluster, albeit in the low priority queue. This provided its own challenge, as I often had to wait overnight for results of a submitted job to be provided, or to find out it errored in some way. I was already starting to become familiar with SLURM, the workload manager used to submit jobs, and my knowledge only improved for needing to use it here as well.

\minititle{Documentation}
The last period of work is producing the documentation, which encompasses both creating and giving a presentation on the completed work, and writing this report. The presentation was made using google slides, and I believe it went well, although the demonstration was perhaps not as thourough as it should have been. The report utlilises LaTeX and B, and also was completed well within the alloted time, allowing for sufficient re-drafting.

\subsubsection{Tools Selection}
I am satisfied with my selection of tools for this project. There was certainly no need to diverge from using GitHub for version control as the rest of the OP2 Framework does, and there have been no issues with using it during this project as I was already very familiar with using \verb|git| prior to starting.
\par
For development, the use of GNU make for AOT compilation is fine enough and very conveniant to combine many commands into a single simple one, however, using it for the JIT compile as well is not an ideal interface for an application developer using OP2.
\par
Lastly, Google Sheets was selected for the presentation because of familiarity, and to ensure changes are automatically saved to a remote in case of loss of data; and LaTeX was used to produce the report.
