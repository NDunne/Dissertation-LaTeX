% !TEX root =  ../report.tex
% !TeX spellcheck = en-GB

\section{Evaluation}
\label{s:eval}

This project was intended as an investigation, and therefore it can certainly be considered successful. The potential for optimisation through JIT compilation has been proved, via the demonstration that the optimisation of defining constants as literal values is able to re-coup the cost of run-time compilation, if the problem size is sufficiently large.
\par
Since the space for optimisation by this technique is relatively small, the problem size does have to be very large to see benefit. However HPC applications do tend to be very large, hence why they are not run on normal computers. This is not a significant restriction.
\par
As a result of the contributions made to the OP2 Framework while completing this project, important groundwork has been laid for future contributors to build on top of. Further optimisations and run-time assertions can be implemented as extensions to this implementation, which might achieve further speedup at run-time for smaller problem sizes.

\subsection{Future Work}
\label{ss:fw}

\subsubsection{Run-Time Assertions}
As previously mentioned, it seems necessary for more assertions to be made at run-time in order to produce more effective speed-up after JIT compilation. \par
There are a number of possible loop optimisations which could be made, including identifying a loop inside a kernel, and at run-time having the loop bound be hard-coded to remove the need to evaluate the expression of every iteration. Another possible optimisation would be Loop Fusion where two separate parallel loops might be provably able to be fused into a single loop, but only if the inputs allow for it - meaning this could only be done at run-time.

\noindent There is also current research into automatically applying loop tiling to the generated code, which is dividing the iterations of a loop into sub-regions where both temporal and spatial locality in memory can be exploited. \par

\begin{wrapfigure}{r}{.4\textwidth}
  \centering
  \caption{2D Loop Tiling}
  \label{fig:tile2D}
  \hspace{-1em}
  \begin{tikzpicture}
      [%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          box/.style={rectangle,draw=black,thick, minimum size=.4cm},
          scale=0.4
      ]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \foreach \x in {0,...,9}{
      \foreach \y in {0,...,9}
          \node[box] at (\x,\y){};
  }

  \foreach \x in {0,...,3}{
      \foreach \y in {6,...,9}
        \node[box,fill=red] at (\x,\y){};
  }

  \foreach \x in {4,...,7}{
      \foreach \y in {6,...,9}
        \node[box,fill=blue] at (\x,\y){};
  }

  \foreach \x in {8,...,9}{
      \foreach \y in {6,...,9}
        \node[box,fill=green] at (\x,\y){};
  }

  \foreach \x in {0,...,3}{
      \foreach \y in {2,...,5}
        \node[box,fill=green] at (\x,\y){};
  }

  \foreach \x in {4,...,7}{
      \foreach \y in {2,...,5}
        \node[box,fill=red] at (\x,\y){};
  }

  \foreach \x in {8,...,9}{
      \foreach \y in {2,...,5}
        \node[box,fill=blue] at (\x,\y){};
  }

  \foreach \x in {0,...,3}{
      \foreach \y in {0,...,1}
        \node[box,fill=blue] at (\x,\y){};
  }

  \foreach \x in {4,...,7}{
      \foreach \y in {0,...,1}
        \node[box,fill=green] at (\x,\y){};
  }

  \foreach \x in {8,...,9}{
      \foreach \y in {0,...,1}
        \node[box,fill=red] at (\x,\y){};
  }

  \draw[<->] (-1,6) -- (-1,9);
  \node[anchor=south east] at (-1,7) {$n$};
  \draw[<->] (-3,0) -- (-3,9);
  \node[anchor=south east] at (-3,4) {$N$};

  \end{tikzpicture}
\end{wrapfigure}
For example, a loop iterating over a 2D array of size $N \times N$, with Level 1 (L1) cache size of $n$, such that $n < N$, would benefit from dividing the array into squares of size at most $n \times n$ (see Figure \ref{fig:tile2D}), as long as this does not violate any data dependencies in the order of operations. Doing so prevents values from being evicted from L1 cache prior to being needed again.\par
Currently this has only been applied to OPS \cite{opstiling}, the precursor to OP2 \cite{opsmain}, which supports structured mesh solvers only, but there does exist a 2019 paper \cite{slope} on automated loop tiling for unstructured meshes, and the issues posed by the need for indirect array accesses. A library is provided which demonstrates the technique \cite{SLOPErep}, including a demo using the same \textit{airfoil} application used for this report.

\subsubsection{CUDA JIT Compilation}
Going in a different direction, the CUDA library does provide an interface for JIT compilation natively, which would allow for re-compilation without requiring a system call to \verb|make| for every loop kernel. System calls can be a significant bottleneck in some cases, and this problem would only compound for applications with a large number of parallel loops. Therefore, using the CUDA JIT compilation system would likely bring down the upfront cost of recompilation. For \textit{airfoil} this re-compile time is very low, so it would not have much impact on the results gathered.
\par
Using CUDA's native JIT compilation pipeline would provide the added benefit that an application developer using OP2 would not have to write the Makefile themselves, as currently as its contents are not generated by the OP2 code generator, but simply relies on the executable producing an error if a Makefile with the correct target does not exist.

\subsubsection{Alternative Hardware Targets}
Finally, there are other hardware targets supported by OP2 which may be able to benefit from Just-In-Time compilation, and since the purpose of OP2 is to provide performance on multiple hardware platforms from a single application code any new optimisation which is found to improve performance should also be ported to other platforms, where it might be able to provide benefit. Users who do not primarily execute applications on NVidia GPU hardware should still have the option to utilise the JIT compilation optimisation.

\clearpage
\subsection{Project Management}
\label{ss:pm}
This Section serves as a reflection on the project as a whole, and how I believe it went. A breakdown of the time spent on each task can be found in Appendix \ref{app:tracker}.
\par
The Gantt chart in figure \ref{progGantt} was produced for the progress report submitted in November, 6 weeks into the project. Having now completed the project I can reflect on how well the timeline was followed, and the successes and challenges of each of the four periods.
\newcommand\w{25}
\begin{figure}[h]
\centering
\caption{\label{progGantt} Gantt Diagram produced with Progress Report}
\makebox[0pt]{
\begin{ganttchart}[
expand chart=1.22\textwidth,
vgrid={*{1}dotted, *4{white},*1{dotted}, *{12}{white}, *1{dotted}, *1{white}, *1{dotted}, *5{white}},
hgrid=true,
y unit chart=0.8cm,
inline,
today=6,
today label=Progress Report,
today label font=\itshape,
]{0}{\w}

 \gantttitle{Timeline}{26} \\
 \gantttitlelist{0,...,\w}{1} \\

 \ganttgroup{Research}{1}{5} \\ %0%

 \ganttset{inline=false}
 \ganttbar{Read Papers}{1}{2}          %1%
 \ganttset{inline=true}

 \ganttgroup{Implementation}{6}{18} \\ %2%

 \ganttset{inline=false}
 \ganttbar{Familiarity Work}{2}{5}     %3%
 \ganttset{inline=true}

 \ganttgroup{Benchmarking}{19}{20} \\  %4%

 \ganttset{inline=false}
 \ganttbar{Feature Development}{6}{8}  %5%
 \ganttbar{} {10}{12}                   %6%
 \ganttbar{} {14}{17}                  %7%
 \ganttset{inline=true}
 \ganttgroup{Documentation}{21}{25} \\ %8%

 \ganttset{inline=false}

 \ganttbar{Feature Testing}{9}{9}      %9%
 \ganttbar{}{13}{13}                   %10%
 \ganttbar{}{18}{18}                   %11%

\\

 \ganttbar{Results Gathering}{19}{19}  %12%
 \\
 \ganttbar{Results Processing}{19}{20} %13%
 \\

 \ganttbar{Final Report}{21}{25} \\    %14
 \ganttbar{Presentation}{21}{22}       %15

 \ganttlink{elem1}{elem3}
 \ganttlink{elem3}{elem5}
 \ganttlink{elem5}{elem9}
 \ganttlink{elem9}{elem6}
 \ganttlink{elem6}{elem10}
 \ganttlink{elem10}{elem7}
 \ganttlink{elem7}{elem11}
 \ganttlink{elem11}{elem12}
 \ganttset{link mid=0.25}
 \ganttlink{elem11}{elem13}
 \ganttset{link mid=0.5}
 \ganttlink{elem13}{elem14}
 \ganttset{link mid=0.25}
 \ganttlink{elem13}{elem15}
 \ganttset{link mid=0.5}
\end{ganttchart}
}
\end{figure}

\subsubsection{Challenges and Reflection}
\hspace{\parindent} \minititle{Research}
The research section of my project involved reading scientific papers, many produced by contributors to the OP2 framework; as well as working hands-on with both CUDA and OP2 to try to build familiarity before the actual implementation began. On reflection, my research was mostly focused on the existing OP2 work, and many of my sources were from the same authors.
\par
Once I had already decided on my approach and begun the implementation I discovered some similar work which might have influenced the direction of the project if I had been aware of it earlier on. For example, the \textit{easy::JIT} approach of performing a code generation stage at runtime as well as compiling is an interesting direction.
\par
Considering that I had never written code for a graphics card before starting this project, I am happy with the confidence in using the CUDA programming model I have built.
\par
The time I had allotted to research did need to be extended from the plan at the outset of the project, as having originally given myself just three weeks I was not confident enough with the existing OP2 code base to begin to contribute. An additional two weeks from the Implementation period were sufficient for me to feel able to begin the implementation.

\minititle{Implementation}
The Implementation progressed largely as expected, with advancements made at a good pace for the time allocated. I did find that while the timetable above listed a total of 3 full weeks for testing, partial solutions were difficult to test fully, as there is no executable generated with which to ensure the result was correct, or perform much benchmarking, until towards the end of the implementation.
\par
Instead, testing during implementation relied more on comparing expected results from the code generation of airfoil, and modified versions of airfoil, with the actual outputs of the code generation scripts. For some areas of development, I found it useful to write the code manually into the \textit{airfoil} files, and figure out how it could work by compiling the manually written code. Once it was compiling successfully, the code generator could be modified to produce equivalent code, but with application specific names replaced with variables so that it would work for any application.
\par
I discussed with my supervisor some extension work which could have been a part of the implementation if there was time, such as the Loop Tiling feature mentioned in the previous Section \ref{ss:fw}. Since the core functionality of the project was only completed with two weeks of planned implementation time left, it was decided that this was unlikely to be completed in the time frame, and that it would be better to thoroughly benchmark the completed functionality than to attempt a complex extension feature and potentially leave it incomplete.

\minititle{Benchmarking}
It was fortunate for the project as a whole that the decision to move on to benchmarking instead of pursuing further functionality was taken, as the original plan allotted only one week for gathering results, and a second week for analysing them. In reality, the extra two weeks that had been allocated to Implementation were also required, as well as an additional week that was supposed to be used for making the project presentation.
\par
The delay was mostly due to the desire to use a HPC cluster to gather proper benchmarking data. While the graphics card in my personal laptop was sufficient for validating the code executed correctly, the results would likely have been noisy and inconsistent if not gathered on a dedicated system. Finding a cluster that could allow me access to a Kepler generation GPU, and getting familiar with using the system once accepted took longer than expected. With the benefit of hindsight it is clear that this process should have begun at the start of the project, as it was always going to be necessary to have access to a HPC cluster, and be familiar with using it by the time the implementation was ready for benchmarking.
\par
Eventually the Cambridge Service for Data-Driven Discovery (CSD3) kindly approved for me to use their \textit{Wilkes2} GPU cluster, with workloads I submitted being placed in a low priority queue. This provided its own challenge, as I often had to wait overnight for results of a submitted job to be provided, or to find out it had failed with some error. I was already starting to become familiar with SLURM from using it as part of a High Performance Computing module, and my knowledge only improved for needing to make use of it here as well.

\minititle{Documentation}
The last period of work is producing the documentation, which encompasses both creating and giving a presentation on the completed work, and writing this report. The presentation was made using Google Slides \cite{gslides}, and I believe it went well, although the demonstration was perhaps not as thorough as it should have been. The benchmarking results I collected for the presentation were only for 10,000 time steps, while the results in this report ran for 1,000,000 - which gave a much better indication of the outcome of applying the optimisation. The total execution time of the application increased from ~30 seconds to ~40 minutes, which clearly allows a small change to amplify into a noticeable difference.
\par
This report was produced as the final submission and utilises LaTeX and BibTeX. It was also completed well within the allotted time, allowing for sufficient re-drafting. The two week extension provided to account for the current pandemic was helpful to make the final stages of the report less stressful to write.

\subsubsection{Tools Selection}
I am satisfied with my selection of tools for this project. There was certainly no need to diverge from using GitHub for version control as the rest of the OP2 Framework does, and there have been no issues with using it during this project as I was already very familiar with using \verb|git| prior to starting. Since there was no collaboration, the workflow is simple and there is only a single branch, with commits whenever a feature or fix is completed.
\par
For development, the use of GNU make for AOT compilation is sufficient,and very convenient to combine many commands into a single, simple one. However, using it for the JIT compile as well is not an ideal interface for an application developer using OP2, as they would need to recreate the contents of my Makefile themselves. It also relies on the OP2 library files, the generated code and the Makefile all being accessible by the binary at run-time, which might not always be the case.
\par
Code was mostly written using Atom \cite{atom} as a text editor, or Vim \cite{vim} when working remotely on the HPC cluster. Atom's GUI is useful when working on a code-base with many files, especially when the output is also a set of files. I used Vim when only a terminal is available, because it is the editor I am most familiar with in that context, and because its popularity means it is usually already available on Linux systems.
\par
Lastly, Google Slides was selected for the presentation because of familiarity, and to ensure changes are automatically saved to a remote in case of loss of data. I prefer LaTeX for producing reports as it allows greater control of the structure of the document than other editors, and provides access to TikZ \cite{tikz}: a powerful package used for producing diagrams.
